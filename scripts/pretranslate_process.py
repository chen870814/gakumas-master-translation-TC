import os
import json
import shutil
import argparse
import datetime

import import_db_json
import export_db_json


def values_to_keys():
    root_dir = input("export æ–‡ä»¶å¤¹: ") or "exports"
    output_dir = "./pretranslate_todo/full_out"

    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    for root, dirs, files in os.walk(root_dir):
        for name in files:
            if not name.endswith(".json"):
                continue

            data = {}
            with open(os.path.join(root, name), 'r', encoding='utf-8') as f:
                orig_data = json.load(f)

            for _, v in orig_data.items():
                data[v] = ""

            with open(os.path.join(output_dir, name), 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=4)

            print("save file", name)


def pretranslated_to_kv_files(
        root_dir: str,
        translated_dir: str,
        save_dir="pretranslate_todo/translated_out"
):
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)

    for root, dirs, files in os.walk(translated_dir):
        for name in files:
            if not name.endswith("_translated.json"):
                continue
            translated_file = os.path.join(root, name)
            print("\n!!! å½“å‰å¤„ç†æ–‡ä»¶ç»å¯¹è·¯å¾„:", os.path.abspath(translated_file))  # æ–°å¢æ­¤è¡Œ
            orig_file = os.path.join(root_dir, name[:-16] + ".json")
            save_file = os.path.join(save_dir, name[:-16] + ".json")

            with open(translated_file, 'r', encoding='utf-8') as f:
                translated_data = json.load(f)  # æ—¥æ–‡: åŸæ–‡

            with open(orig_file, 'r', encoding='utf-8') as f:
                orig_data = json.load(f)  # key: æ—¥æ–‡

            for k, orig_jp in orig_data.items():
                orig_data[k] = translated_data.get(orig_jp, orig_jp)

            with open(save_file, 'w', encoding='utf-8') as f:
                json.dump(orig_data, f, ensure_ascii=False, indent=4)

            print("åˆå¹¶æ–‡ä»¶", name)
    print("åˆå¹¶å®Œæˆï¼Œæ¥ä¸‹æ¥è¯·æ‰§è¡Œ import_db_json å°†ç¿»è¯‘æ–‡ä»¶å¯¼å›")


def backup_temp_key_jp():
    """
    å¤‡ä»½å½“å‰çš„ temp_key_jp åˆ° temp_key_jp_old
    """
    temp_key_jp_dir = "./pretranslate_todo/temp_key_jp"
    temp_key_jp_old_dir = "./pretranslate_todo/temp_key_jp_old"
    
    if not os.path.exists(temp_key_jp_dir):
        print("âŒ é”™è¯¯ï¼štemp_key_jp ç›®å½•ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œæ•°æ®æ›´æ–°æ“ä½œ")
        return False
    
    print("ğŸ“ å¼€å§‹å¤‡ä»½å½“å‰ temp_key_jp åˆ° temp_key_jp_old...")
    
    # å¦‚æœæ—§å¤‡ä»½å­˜åœ¨ï¼Œå…ˆåˆ é™¤
    if os.path.exists(temp_key_jp_old_dir):
        print("ğŸ—‘ï¸  åˆ é™¤æ—§çš„å¤‡ä»½ç›®å½•...")
        shutil.rmtree(temp_key_jp_old_dir)
    
    # å¤åˆ¶ç›®å½•
    try:
        shutil.copytree(temp_key_jp_dir, temp_key_jp_old_dir)
        print("âœ… å¤‡ä»½å®Œæˆï¼")
        return True
    except Exception as e:
        print(f"âŒ å¤‡ä»½å¤±è´¥ï¼š{e}")
        return False


def gen_todo(new_files_dir: str):
    """
    ç”Ÿæˆæœªç¿»è¯‘è¿‡çš„ jp: "" æ–‡ä»¶
    """
    old_files_dir = "./data"
    temp_key_cn_dir = "./pretranslate_todo/temp_key_cn"
    temp_key_jp_dir = "./pretranslate_todo/temp_key_jp"
    temp_key_jp_old_dir = "./pretranslate_todo/temp_key_jp_old"  # æ·»åŠ æ—§ç‰ˆæœ¬ç›®å½•
    todo_out_dir = "./pretranslate_todo/todo"
    changed_out_dir = "./pretranslate_todo/todo/changed"  # å˜åŒ–æ–‡ä»¶è¾“å‡ºç›®å½•
    
    # æ£€æŸ¥ temp_key_jp_old ç›®å½•æ˜¯å¦å­˜åœ¨
    if not os.path.exists(temp_key_jp_old_dir):
        print("âš ï¸  è­¦å‘Šï¼štemp_key_jp_old ç›®å½•ä¸å­˜åœ¨ï¼")
        print("ğŸ” è¿™æ„å‘³ç€æ—¥æ–‡å€¼å˜åŒ–æ£€æµ‹åŠŸèƒ½å°†æ— æ³•å·¥ä½œ")
        print("ğŸ“ åªèƒ½æ£€æµ‹æ–°å¢çš„é”®ï¼Œæ— æ³•æ£€æµ‹å·²æœ‰é”®çš„æ—¥æ–‡å€¼å˜åŒ–")
        print("ğŸ’¡ å»ºè®®æ‰§è¡Œä»¥ä¸‹æ“ä½œä¹‹ä¸€ï¼š")
        print("   1. è¿è¡Œ 'make backup' æˆ– 'python scripts/pretranslate_process.py --backup' æ¥åˆ›å»ºå¤‡ä»½")
        print("   2. å¦‚æœè¿™æ˜¯é¦–æ¬¡è¿è¡Œï¼Œå¯ä»¥å¿½ç•¥æ­¤è­¦å‘Š")
        
        user_choice = input("æ˜¯å¦ç»§ç»­æ‰§è¡Œï¼Ÿ(y/N): ").lower().strip()
        if user_choice not in ['y', 'yes']:
            print("âŒ æ“ä½œå·²å–æ¶ˆ")
            return
        
        print("âš ï¸  ç»§ç»­æ‰§è¡Œï¼Œä½†æ—¥æ–‡å€¼å˜åŒ–æ£€æµ‹åŠŸèƒ½å·²ç¦ç”¨")
    
    # åˆ›å»ºæ—¥å¿—æ–‡ä»¶
    log_file = f"./pretranslate_todo/jp_changes_log_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
    changes_log = []
    changed_files = {}  # å­˜å‚¨æ¯ä¸ªæ–‡ä»¶çš„å˜åŒ– {æ–‡ä»¶å: {(æ—§å€¼, æ–°å€¼): æ—§ç¿»è¯‘}}

    if not os.path.isdir(temp_key_cn_dir):
        os.makedirs(temp_key_cn_dir)
    if not os.path.isdir(temp_key_jp_dir):
        os.makedirs(temp_key_jp_dir)
    if not os.path.isdir(todo_out_dir):
        os.makedirs(todo_out_dir)
    if not os.path.isdir(changed_out_dir):
        os.makedirs(changed_out_dir)

    # æ—§å·²ç¿»è¯‘æ’ä»¶ json è½¬ key: cn
    for root, dirs, files in os.walk(old_files_dir):
        for file in files:
            if file.endswith(".json"):
                input_path = os.path.join(root, file)
                output_path = os.path.join(temp_key_cn_dir, file)
                export_db_json.ex_main(input_path, output_path)

    # æ–°æ’ä»¶ json è½¬ key: jp
    for root, dirs, files in os.walk(new_files_dir):
        for file in files:
            if file.endswith(".json"):
                input_path = os.path.join(root, file)
                output_path = os.path.join(temp_key_jp_dir, file)
                export_db_json.ex_main(input_path, output_path)

                # ç‰¹ä¾‹å¤„ç†ï¼šProduceCardCustomize.json
                if file == "ProduceCardCustomize.json":
                    with open(input_path, 'r', encoding='utf-8') as f:
                        jp_data = json.load(f)

                    # è®€å–åŸå§‹ data é™£åˆ—ï¼Œçµ„åˆæ‰å¹³åŒ– key
                    with open(output_path, 'r', encoding='utf-8') as f:
                        out_data = json.load(f)

                    for entry in jp_data.get("data", []):
                        composite_key = f"{entry['id']}|{entry['customizeCount']}|description"
                        # å³ä½¿ description æ˜¯ç©ºå­—ä¸²ä¹Ÿè¦å¯«å…¥
                        out_data[composite_key] = entry.get("description", "")

                    # è¦†å¯«å›è¼¸å‡ºæª”æ¡ˆ
                    with open(output_path, 'w', encoding='utf-8') as f:
                        json.dump(out_data, f, ensure_ascii=False, indent=4)

                    print(f"ç‰¹ä¾‹è™•ç†å®Œæˆ: {output_path}")

    # éå†æ–°çš„ jp æ–‡ä»¶
    for root, dirs, files in os.walk(temp_key_jp_dir):
        for file in files:
            jp_file = os.path.join(root, file)
            jp_old_file = os.path.join(temp_key_jp_old_dir, file)  # æ—§ç‰ˆæœ¬æ—¥æ–‡æ–‡ä»¶
            cn_file = os.path.join(temp_key_cn_dir, file)
            out_data = {}

            with open(jp_file, 'r', encoding='utf-8') as f:
                jp_data = json.load(f)

            # åŠ è½½æ—§ç‰ˆæœ¬çš„æ—¥æ–‡æ•°æ®
            jp_old_data = {}
            if os.path.exists(temp_key_jp_old_dir) and os.path.exists(jp_old_file):
                with open(jp_old_file, 'r', encoding='utf-8') as f:
                    jp_old_data = json.load(f)
            
            if not os.path.exists(cn_file):
                # å¦‚æœæ²¡æœ‰æ—§çš„ç¿»è¯‘æ–‡ä»¶ï¼Œæ‰€æœ‰æ—¥æ–‡éƒ½éœ€è¦ç¿»è¯‘
                for _, v in jp_data.items():
                    out_data[v] = ""
            else:
                with open(cn_file, 'r', encoding='utf-8') as f:
                    cn_data = json.load(f)
                for k, v in jp_data.items():
                    # æ£€æŸ¥æ¡ä»¶ï¼š
                    # 1. é”®ä¸å­˜åœ¨äºæ—§ç¿»è¯‘ä¸­ (æ–°å¢çš„é”®)
                    # 2. é”®å­˜åœ¨äºæ—§ç¿»è¯‘ä¸­ï¼Œä½†æ—¥æ–‡å€¼å‘ç”Ÿäº†å˜åŒ– (å€¼æ›´æ–°çš„é”®)
                    if k not in cn_data:
                        # æ–°å¢çš„é”®ï¼Œç›´æ¥æ·»åŠ 
                        out_data[v] = ""
                    elif k in jp_old_data and jp_old_data[k] != v and k in cn_data:
                        # é”®å­˜åœ¨ï¼Œæ—¥æ–‡å€¼å‘ç”Ÿå˜åŒ–ï¼Œä¸”ä¹‹å‰æœ‰ç¿»è¯‘
                        change_info = f"æ–‡ä»¶: {file}\né”®: {k}\næ—§å€¼: {jp_old_data[k]}\næ–°å€¼: {v}\nåŸç¿»è¯‘: {cn_data[k]}\n{'='*50}"
                        changes_log.append(change_info)
                        # è®°å½•åˆ°å˜åŒ–æ–‡ä»¶å­—å…¸ï¼ˆå»é‡ç›¸åŒçš„æ—§å€¼->æ–°å€¼å˜åŒ–ï¼‰
                        if file not in changed_files:
                            changed_files[file] = {}
                        change_key = (jp_old_data[k], v)
                        if change_key not in changed_files[file]:
                            changed_files[file][change_key] = cn_data[k]
                        print(f"æ£€æµ‹åˆ°æ—¥æ–‡å€¼å˜åŒ–: {file} - {k}")
                        print(f"  æ—§å€¼: {jp_old_data[k]}")
                        print(f"  æ–°å€¼: {v}")
                        print(f"  åŸç¿»è¯‘: {cn_data[k]}")
                        out_data[v] = ""
            if out_data:
                todo_file = os.path.join(todo_out_dir, file)
                with open(todo_file, 'w', encoding='utf-8') as f:
                    json.dump(out_data, f, ensure_ascii=False, indent=4)
                print("TODO File", todo_file)
    
    # ä¿å­˜å˜åŒ–çš„æ–‡ä»¶åˆ° changed ç›®å½•ï¼ˆCSVæ ¼å¼ï¼‰
    for file_name, changes in changed_files.items():
        changed_file_path = os.path.join(changed_out_dir, file_name.replace('.json', '.csv'))
        with open(changed_file_path, 'w', encoding='utf-8', newline='') as f:
            # æ‰‹åŠ¨å†™å…¥CSVæ ¼å¼ï¼Œå®Œå…¨ä¿æŒåŸæ ·
            f.write('æ—§å€¼,æ–°å€¼,æ—§ç¿»è¯‘,æ–°ç¿»è¯‘\n')
            for (old_value, new_value), old_translation in changes.items():
                # ç›´æ¥æ‹¼æ¥ï¼Œå®Œå…¨æŒ‰åŸæ ·ä¿å­˜
                line = f'{old_value},{new_value},{old_translation},\n'
                f.write(line)
        print(f"å˜åŒ–æ–‡ä»¶å·²ä¿å­˜: {changed_file_path} (åŒ…å« {len(changes)} ä¸ªå”¯ä¸€å˜åŒ–)")
    
    # ä¿å­˜å˜åŒ–æ—¥å¿—
    if changes_log:
        with open(log_file, 'w', encoding='utf-8') as f:
            f.write(f"æ—¥æ–‡å€¼å˜åŒ–æ£€æµ‹æŠ¥å‘Š\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"æ£€æµ‹åˆ° {len(changes_log)} å¤„å˜åŒ–\n")
            f.write("="*80 + "\n\n")
            f.write("\n\n".join(changes_log))
        print(f"\nå˜åŒ–æ—¥å¿—å·²ä¿å­˜åˆ°: {log_file}")
        print(f"å…±æ£€æµ‹åˆ° {len(changes_log)} å¤„æ—¥æ–‡å€¼å˜åŒ–")
        print(f"å˜åŒ–æ–‡ä»¶å·²ä¿å­˜åˆ°: {changed_out_dir}")
    else:
        print("\næœªæ£€æµ‹åˆ°æ—¥æ–‡å€¼å˜åŒ–")
    
    # æ·»åŠ æœ€ç»ˆçŠ¶æ€æŠ¥å‘Š
    if not os.path.exists(temp_key_jp_old_dir):
        print("\nğŸ“Š æ‰§è¡ŒçŠ¶æ€æŠ¥å‘Šï¼š")
        print("âœ… æ–°å¢é”®æ£€æµ‹ï¼šå·²æ‰§è¡Œ")
        print("âŒ æ—¥æ–‡å€¼å˜åŒ–æ£€æµ‹ï¼šå·²è·³è¿‡ï¼ˆç¼ºå°‘å¤‡ä»½ç›®å½•ï¼‰")
        print("ğŸ’¡ ä¸‹æ¬¡è¿è¡Œå‰å»ºè®®å…ˆæ‰§è¡Œå¤‡ä»½æ“ä½œ")


def merge_todo():
    new_files_dir = "./pretranslate_todo/todo/new"  # åªæœ‰æ–°çš„ jp: cn
    old_trans_dir = "./pretranslate_todo/temp_key_cn"  # æ—§ç‰ˆ key: cn
    new_key_jp_dir = "./pretranslate_todo/temp_key_jp"  # æ–°ç‰ˆ key: jp
    output_dir = "./pretranslate_todo/merged"  # æ–°çš„ key: cn

    if not os.path.isdir(output_dir):
        os.makedirs(output_dir)

    # é¦–å…ˆå°†æ–°çš„ key: jp å¤åˆ¶åˆ°è¾“å‡ºæ–‡ä»¶å¤¹
    for root, dirs, files in os.walk(new_key_jp_dir):
        for file in files:
            if file.endswith(".json"):
                shutil.copyfile(os.path.join(root, file), os.path.join(output_dir, file))

    # åˆå¹¶æ—§ç¿»è¯‘
    for root, dirs, files in os.walk(old_trans_dir):
        for file in files:
            if file.endswith(".json"):
                old_key_cn_file = os.path.join(root, file)  # æ—§ç‰ˆ key: cn
                new_key_jp_file = os.path.join(output_dir, file)  # ç›®å‰ output_dir æ˜¯æ–°ç‰ˆ key: jp

                with open(old_key_cn_file, 'r', encoding='utf-8') as f:
                    old_key_cn_data: dict = json.load(f)
                if os.path.exists(new_key_jp_file):
                    with open(new_key_jp_file, 'r', encoding='utf-8') as f:
                        new_key_jp_data = json.load(f)
                else:
                    new_key_jp_data = {}

                for k, v in old_key_cn_data.items():
                    new_key_jp_data[k] = v

                with open(new_key_jp_file, 'w', encoding='utf-8') as f:
                    json.dump(new_key_jp_data, f, ensure_ascii=False, indent=4)

    pretranslated_to_kv_files(output_dir, new_files_dir, output_dir)

    if input("ç»§ç»­æ‰§è¡Œ import_db_jsonï¼Œè¯·è¾“å…¥ 1: ") == "1":
        import_db_json.main("./gakumasu-diff/json", output_dir, "data")
        print("æ–‡ä»¶å·²è¾“å‡ºåˆ° data")


def apply_changed_translations():
    """
    åº”ç”¨ changed æ–‡ä»¶å¤¹ä¸­çš„æ–°ç¿»è¯‘åˆ° temp_key_cn å’Œ jp_cn æ–‡ä»¶å¤¹
    """
    changed_dir = "./pretranslate_todo/todo/changed"
    temp_key_cn_dir = "./pretranslate_todo/temp_key_cn"
    temp_key_jp_dir = "./pretranslate_todo/temp_key_jp"
    jp_cn_dir = "./pretranslate_todo/jp_cn"
    
    if not os.path.exists(changed_dir):
        print("âŒ é”™è¯¯ï¼šchanged æ–‡ä»¶å¤¹ä¸å­˜åœ¨")
        return False
    
    if not os.path.exists(temp_key_jp_dir):
        print("âŒ é”™è¯¯ï¼štemp_key_jp æ–‡ä»¶å¤¹ä¸å­˜åœ¨")
        return False
    
    # ç¡®ä¿ç›®æ ‡æ–‡ä»¶å¤¹å­˜åœ¨
    if not os.path.exists(temp_key_cn_dir):
        os.makedirs(temp_key_cn_dir)
    if not os.path.exists(jp_cn_dir):
        os.makedirs(jp_cn_dir)
    
    print("ğŸ”„ å¼€å§‹å¤„ç† changed æ–‡ä»¶å¤¹ä¸­çš„ç¿»è¯‘æ›´æ–°...")
    
    updated_count = 0
    processed_files = 0
    
    # éå† changed æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰ CSV æ–‡ä»¶
    for filename in os.listdir(changed_dir):
        if not filename.endswith('.csv'):
            continue
            
        csv_file_path = os.path.join(changed_dir, filename)
        json_filename = filename.replace('.csv', '.json')
        
        print(f"\nğŸ“„ å¤„ç†æ–‡ä»¶: {filename}")
        
        # è¯»å–å¯¹åº”çš„ temp_key_jp æ–‡ä»¶
        temp_key_jp_file = os.path.join(temp_key_jp_dir, json_filename)
        if not os.path.exists(temp_key_jp_file):
            print(f"âš ï¸  è­¦å‘Šï¼šæ‰¾ä¸åˆ°å¯¹åº”çš„ temp_key_jp æ–‡ä»¶: {json_filename}")
            continue
        
        with open(temp_key_jp_file, 'r', encoding='utf-8') as f:
            temp_key_jp_data = json.load(f)
        
        # è¯»å–æˆ–åˆ›å»ºå¯¹åº”çš„ temp_key_cn æ–‡ä»¶
        temp_key_cn_file = os.path.join(temp_key_cn_dir, json_filename)
        if os.path.exists(temp_key_cn_file):
            with open(temp_key_cn_file, 'r', encoding='utf-8') as f:
                temp_key_cn_data = json.load(f)
        else:
            temp_key_cn_data = {}
        
        # è¯»å–æˆ–åˆ›å»ºå¯¹åº”çš„ jp_cn æ–‡ä»¶
        jp_cn_file = os.path.join(jp_cn_dir, json_filename)
        if os.path.exists(jp_cn_file):
            with open(jp_cn_file, 'r', encoding='utf-8') as f:
                jp_cn_data = json.load(f)
        else:
            jp_cn_data = {}
        
        # å¤„ç† CSV æ–‡ä»¶
        file_updated_count = 0
        try:
            with open(csv_file_path, 'r', encoding='utf-8') as f:
                # æ‰‹åŠ¨è§£æCSVï¼Œé¿å…å¤æ‚çš„è½¬ä¹‰é—®é¢˜
                lines = f.readlines()
                if len(lines) <= 1:  # åªæœ‰æ ‡é¢˜è¡Œæˆ–ç©ºæ–‡ä»¶
                    continue
                
                for line_num, line in enumerate(lines[1:], 2):  # è·³è¿‡æ ‡é¢˜è¡Œ
                    line = line.strip()
                    if not line:
                        continue
                    
                    # è§£æCSVè¡Œï¼šæ—§å€¼,æ–°å€¼,æ—§ç¿»è¯‘,æ–°ç¿»è¯‘
                    parts = line.split(',')
                    if len(parts) < 4:
                        print(f"âš ï¸  ç¬¬{line_num}è¡Œæ ¼å¼é”™è¯¯ï¼Œè·³è¿‡: {line}")
                        continue
                    
                    # å¤„ç†å¯èƒ½åŒ…å«é€—å·çš„å­—æ®µï¼ˆç®€å•å¤„ç†ï¼‰
                    if len(parts) > 4:
                        # å‡è®¾åªæœ‰æœ€åä¸€ä¸ªå­—æ®µï¼ˆæ–°ç¿»è¯‘ï¼‰å¯èƒ½åŒ…å«é€—å·
                        old_value = parts[0]
                        new_value = parts[1]
                        old_translation = parts[2]
                        new_translation = ','.join(parts[3:])
                    else:
                        old_value, new_value, old_translation, new_translation = parts
                    
                    # ä¸åšä»»ä½•è½¬ä¹‰å¤„ç†ï¼Œå®Œå…¨æŒ‰åŸæ ·ä½¿ç”¨
                    
                    # å¦‚æœæ–°ç¿»è¯‘ä¸ºç©ºï¼Œè·³è¿‡
                    if not new_translation.strip():
                        continue
                    
                    # è¯»å–å¯¹åº”çš„æ—§ç‰ˆæœ¬ temp_key_jp æ–‡ä»¶æ¥ç²¾ç¡®åŒ¹é…å˜åŒ–çš„é”®
                    temp_key_jp_old_file = os.path.join("./pretranslate_todo/temp_key_jp_old", json_filename)
                    if not os.path.exists(temp_key_jp_old_file):
                        print(f"âš ï¸  è­¦å‘Šï¼šæ‰¾ä¸åˆ°æ—§ç‰ˆæœ¬æ–‡ä»¶ï¼Œæ— æ³•ç²¾ç¡®åŒ¹é…å˜åŒ–: {json_filename}")
                        continue
                    
                    with open(temp_key_jp_old_file, 'r', encoding='utf-8') as f:
                        temp_key_jp_old_data = json.load(f)
                    
                    # æ‰¾åˆ°ç¡®å®å‘ç”Ÿå˜åŒ–çš„é”®ï¼šæ—§å€¼ -> æ–°å€¼
                    matching_keys = []
                    for key, old_jp_value in temp_key_jp_old_data.items():
                        if (old_jp_value == old_value and 
                            key in temp_key_jp_data and 
                            temp_key_jp_data[key] == new_value):
                            matching_keys.append(key)
                    
                    if not matching_keys:
                        print(f"âš ï¸  æ‰¾ä¸åˆ°ä»'{old_value[:20]}...'å˜ä¸º'{new_value[:20]}...'çš„é”®")
                        continue
                    
                    # æ›´æ–°æ‰€æœ‰åŒ¹é…çš„é”®
                    for key in matching_keys:
                        # æ›´æ–° temp_key_cn
                        temp_key_cn_data[key] = new_translation
                        file_updated_count += 1
                    
                    # æ›´æ–° jp_cnï¼ˆåªåœ¨é”®ä¸å­˜åœ¨æ—¶æ·»åŠ æ–°çš„æ˜ å°„ï¼‰
                    if new_value not in jp_cn_data:
                        jp_cn_data[new_value] = new_translation
                    elif jp_cn_data[new_value] != new_translation:
                        # å¦‚æœé”®å­˜åœ¨ä½†å€¼ä¸åŒï¼Œæ›´æ–°ä¸ºæ–°ç¿»è¯‘
                        jp_cn_data[new_value] = new_translation
                    
                    print(f"âœ… æ›´æ–°ç¿»è¯‘ ({len(matching_keys)}ä¸ªé”®): '{old_value[:20]}...' -> '{new_value[:20]}...' = '{new_translation[:20]}...'")
        
        except Exception as e:
            print(f"âŒ å¤„ç† CSV æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            continue
        
        # ä¿å­˜æ›´æ–°åçš„æ–‡ä»¶
        if file_updated_count > 0:
            with open(temp_key_cn_file, 'w', encoding='utf-8') as f:
                json.dump(temp_key_cn_data, f, ensure_ascii=False, indent=2)
            
            with open(jp_cn_file, 'w', encoding='utf-8') as f:
                json.dump(jp_cn_data, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ å·²ä¿å­˜ {file_updated_count} ä¸ªç¿»è¯‘æ›´æ–°åˆ° {json_filename}")
            updated_count += file_updated_count
            processed_files += 1
        else:
            print(f"â„¹ï¸  {json_filename} æ²¡æœ‰éœ€è¦æ›´æ–°çš„ç¿»è¯‘")
    
    print(f"\nğŸ‰ å¤„ç†å®Œæˆï¼")
    print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯ï¼š")
    print(f"   - å¤„ç†çš„æ–‡ä»¶æ•°: {processed_files}")
    print(f"   - æ›´æ–°çš„ç¿»è¯‘æ•°: {updated_count}")
    
    if updated_count > 0:
        print(f"\nğŸ’¡ æç¤ºï¼šç¿»è¯‘å·²æ›´æ–°åˆ°ä»¥ä¸‹æ–‡ä»¶å¤¹ï¼š")
        print(f"   - temp_key_cn: {temp_key_cn_dir}")
        print(f"   - jp_cn: {jp_cn_dir}")
        print(f"   ä½ å¯ä»¥ç»§ç»­æ‰§è¡Œåç»­çš„åˆå¹¶æµç¨‹")
    
    return True


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--gen_todo', action='store_true')
    parser.add_argument('--merge', action='store_true')
    parser.add_argument('--backup', action='store_true', help='å¤‡ä»½å½“å‰çš„ temp_key_jp')
    parser.add_argument('--apply_changed', action='store_true', help='åº”ç”¨ changed æ–‡ä»¶å¤¹ä¸­çš„æ–°ç¿»è¯‘')
    args = parser.parse_args()

    if args.backup:
        backup_temp_key_jp()
        return

    if args.apply_changed:
        apply_changed_translations()
        return

    if (not args.gen_todo) and (not args.merge):
        do_idx = input("[1] å…¨éƒ¨å¯¼å‡ºè½¬ä¸ºå¾…ç¿»è¯‘æ–‡ä»¶\n"
                       "[2] å¯¹æ¯”æ›´æ–°ç—…ç”Ÿæˆ todo æ–‡ä»¶\n"
                       "[3] ç¿»è¯‘æ–‡ä»¶(jp: cn)è½¬å› key-value json\n"
                       "[4] å°†ç¿»è¯‘åçš„ todo æ–‡ä»¶åˆå¹¶å›æ’ä»¶ json\n"
                       "[5] å¤‡ä»½å½“å‰ temp_key_jp\n"
                       "[6] åº”ç”¨ changed æ–‡ä»¶å¤¹ä¸­çš„æ–°ç¿»è¯‘\n"
                       "è¯·é€‰æ‹©æ“ä½œ: ")
    elif args.gen_todo:
        gen_todo("gakumasu-diff/json")
        return
    elif args.merge:
        do_idx = "4"
    else:
        raise RuntimeError("Invalid Arguments.")

    if do_idx == "1":
        values_to_keys()

    elif do_idx == "2":
        gen_todo(input("æ–° gakumasu_diff_to_json æ–‡ä»¶å¤¹: ") or "gakumasu-diff/json")

    elif do_idx == "3":
        pretranslated_to_kv_files(
            root_dir=input("export æ–‡ä»¶å¤¹: ") or "exports",
            translated_dir=input("é¢„ç¿»è¯‘å®Œæˆæ–‡ä»¶å¤¹: ")
        )

    elif do_idx == "4":
        merge_todo()

    elif do_idx == "5":
        backup_temp_key_jp()

    elif do_idx == "6":
        apply_changed_translations()


if __name__ == '__main__':
    main()
