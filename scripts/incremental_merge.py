#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import json
import shutil
import sys
import csv
from datetime import datetime

# æ·»åŠ  scripts ç›®å½•åˆ°è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥å…¶ä»–æ¨¡å—
sys.path.append(os.path.join(os.path.dirname(__file__), 'scripts'))

import import_db_json
import export_db_json


def get_todo_new_files():
    """èŽ·å– todo/new ä¸­çš„æ–‡ä»¶åˆ—è¡¨"""
    new_files_dir = "./pretranslate_todo/todo/new"
    todo_files = []
    
    if os.path.exists(new_files_dir):
        for root, dirs, files in os.walk(new_files_dir):
            for file in files:
                if file.endswith("_translated.json"):
                    json_name = file[:-16] + ".json"
                    todo_files.append(json_name)
    
    return todo_files


def pretranslated_to_kv_files_single(root_dir: str, translated_file: str, json_filename: str):
    """å°†å•ä¸ªç¿»è¯‘æ–‡ä»¶è½¬æ¢ä¸º key-value æ ¼å¼"""
    temp_output = {}
    
    with open(translated_file, 'r', encoding='utf-8') as f:
        translated_data = json.load(f)  # æ—¥æ–‡: ä¸­æ–‡

    orig_file = os.path.join(root_dir, json_filename)
    if os.path.exists(orig_file):
        with open(orig_file, 'r', encoding='utf-8') as f:
            orig_data = json.load(f)  # key: æ—¥æ–‡

        for k, orig_jp in orig_data.items():
            temp_output[k] = translated_data.get(orig_jp, orig_jp)
    
    return temp_output


def load_special_mappings(json_filename):
    """åŠ è½½special_mappingç›®å½•ä¸­çš„ç‰¹æ®Šæ˜ å°„"""
    special_mapping_dir = "./special_mapping"
    file_base_name = json_filename[:-5]  # ç§»é™¤.jsonæ‰©å±•å
    
    special_mappings = {}
    special_keys = set()  # ç”¨äºŽå¿«é€Ÿæ£€æŸ¥æ˜¯å¦ä¸ºç‰¹ä¾‹é”®
    
    # æŸ¥æ‰¾å¯¹åº”çš„special_mappingå­ç›®å½•
    category_dir = os.path.join(special_mapping_dir, file_base_name)
    if os.path.exists(category_dir):
        for special_file in os.listdir(category_dir):
            if special_file.endswith('.json'):
                special_file_path = os.path.join(category_dir, special_file)
                try:
                    with open(special_file_path, 'r', encoding='utf-8') as f:
                        special_data = json.load(f)
                        special_mappings.update(special_data)
                        # è®°å½•ç‰¹ä¾‹é”®å‰ç¼€ï¼ˆå¦‚pitem_01-2-096-0ï¼‰
                        special_prefix = special_file[:-5]  # ç§»é™¤.json
                        special_keys.add(special_prefix)
                        print(f"    ðŸ“‹ åŠ è½½ç‰¹æ®Šæ˜ å°„: {special_file} ({len(special_data)} æ¡)")
                except Exception as e:
                    print(f"    âš ï¸  åŠ è½½ç‰¹æ®Šæ˜ å°„å¤±è´¥ {special_file}: {e}")
    
    return special_mappings, special_keys


def extract_complete_item_from_source(json_filename, item_id, source_dir="./gakumasu-diff/json"):
    """ä»Žæºæ–‡ä»¶ä¸­æå–å®Œæ•´çš„æ¡ç›®ä¿¡æ¯"""
    source_file = os.path.join(source_dir, json_filename)
    if not os.path.exists(source_file):
        print(f"    âš ï¸  æ‰¾ä¸åˆ°æºæ–‡ä»¶: {source_file}")
        return {}
    
    try:
        with open(source_file, 'r', encoding='utf-8') as f:
            source_data = json.load(f)
        
        # æå–è¯¥item_idçš„æ‰€æœ‰ç›¸å…³é”®å€¼å¯¹
        item_mappings = {}
        for key, value in source_data.items():
            if key.startswith(item_id + "|"):
                item_mappings[key] = value
        
        return item_mappings
    except Exception as e:
        print(f"    âš ï¸  è¯»å–æºæ–‡ä»¶å¤±è´¥ {source_file}: {e}")
        return {}


def save_smart_resolved_exceptions(json_filename, smart_exceptions):
    """
    ä¿å­˜æ™ºèƒ½è§£å†³çš„ç‰¹ä¾‹åˆ°special_mapping
    
    TODO: å½“gakumasu-diffæœ‰æ›´æ–°æ—¶ï¼Œéœ€è¦å®Œå–„ä»¥ä¸‹é€»è¾‘ï¼š
    1. ä»Žgakumasu-diff/jsonä¸­æå–å®Œæ•´çš„æ–°å¢žæ¡ç›®
    2. å¯¹æ¯”dataå’Œdiffï¼Œç¡®å®šå“ªäº›æ˜¯çœŸæ­£çš„æ–°å¢žå†…å®¹
    3. ä¸ºæ¯ä¸ªæ¡ç›®åˆ›å»ºå®Œæ•´çš„special mappingæ–‡ä»¶
    
    å½“å‰ç‰ˆæœ¬ï¼šä»…åŸºäºŽå†²çªé”®åˆ›å»ºæ˜ å°„ï¼Œå¾…çœŸå®žæ•°æ®éªŒè¯åŽå®Œå–„
    """
    if not smart_exceptions:
        return
        
    special_mapping_dir = "./special_mapping"
    file_base_name = json_filename[:-5]  # ç§»é™¤.jsonæ‰©å±•å
    category_dir = os.path.join(special_mapping_dir, file_base_name)
    
    # æŒ‰æ¡ç›®IDåˆ†ç»„
    exceptions_by_item = {}
    for exception in smart_exceptions:
        key = exception["key"]
        # æå–æ¡ç›®IDï¼ˆä¾‹å¦‚ä»Žpitem_tower_001-exam_review-1-stage_022-1|...ä¸­æå–pitem_tower_001-exam_review-1-stage_022-1ï¼‰
        item_id = key.split("|")[0]
        if item_id not in exceptions_by_item:
            exceptions_by_item[item_id] = []
        exceptions_by_item[item_id].append(exception)
    
    # ä¸ºæ¯ä¸ªæ¡ç›®åˆ›å»ºå®Œæ•´çš„special mapping
    for item_id, exceptions in exceptions_by_item.items():
        print(f"    ðŸ” å¤„ç†ç‰¹ä¾‹æ¡ç›®: {item_id}")
        
        # ä»Žgakumasu-diffä¸­æå–å®Œæ•´çš„æ¡ç›®
        complete_mappings = extract_complete_item_from_source(json_filename, item_id)
        
        if not complete_mappings:
            print(f"    âš ï¸  æ— æ³•æ‰¾åˆ°æ¡ç›® {item_id} çš„å®Œæ•´ä¿¡æ¯")
            continue
        
        # åº”ç”¨todo/newçš„ç‰¹ä¾‹ç¿»è¯‘åˆ°å®Œæ•´æ˜ å°„ä¸­
        for exception in exceptions:
            key = exception["key"]
            if key in complete_mappings:
                complete_mappings[key] = exception["todo_new_translation"]
        
        # åˆ›å»ºç›®å½•
        if not os.path.exists(category_dir):
            os.makedirs(category_dir)
            
        # ä¿å­˜special mappingæ–‡ä»¶
        special_file = os.path.join(category_dir, f"{item_id}.json")
        with open(special_file, 'w', encoding='utf-8') as f:
            json.dump(complete_mappings, f, ensure_ascii=False, indent=2)
        
        print(f"    ðŸ’¾ å·²åˆ›å»ºç‰¹ä¾‹æ˜ å°„: {item_id}.json ({len(complete_mappings)} æ¡å®Œæ•´æ˜ å°„)")


def is_special_key(key, special_keys):
    """æ£€æŸ¥é”®æ˜¯å¦å±žäºŽç‰¹æ®Šæ˜ å°„"""
    for special_prefix in special_keys:
        if key.startswith(special_prefix):
            return True
    return False


def incremental_merge():
    """
    å¢žé‡åˆå¹¶æµç¨‹ï¼š
    1. åªå¤„ç† todo/new ä¸­å­˜åœ¨çš„æ–‡ä»¶
    2. å¯¹è¿™äº›æ–‡ä»¶è¿›è¡Œä¼˜å…ˆçº§åˆå¹¶ï¼štodo/new > jp_cn > temp_key_cn(data)
    3. ç”Ÿæˆå†²çªæŠ¥å‘ŠCSV
    4. åªæ›´æ–°å¯¹åº”çš„ data æ–‡ä»¶
    """
    new_files_dir = "./pretranslate_todo/todo/new"
    old_trans_dir = "./pretranslate_todo/temp_key_cn"
    new_key_jp_dir = "./pretranslate_todo/temp_key_jp"
    jp_cn_dir = "./pretranslate_todo/jp_cn"
    output_dir = "./pretranslate_todo/merged"
    conflicts_dir = "./pretranslate_todo/conflicts"
    data_dir = "./data"
    gakumasu_json_dir = "./gakumasu-diff/json"

    # åˆ›å»ºå¿…è¦çš„ç›®å½•
    for dir_path in [output_dir, conflicts_dir]:
        if not os.path.isdir(dir_path):
            os.makedirs(dir_path)

    print("ðŸ”„ å¼€å§‹å¢žé‡åˆå¹¶æµç¨‹...")
    
    # 1. èŽ·å–éœ€è¦å¤„ç†çš„æ–‡ä»¶åˆ—è¡¨
    todo_files = get_todo_new_files()
    if not todo_files:
        print("âŒ todo/new ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°ç¿»è¯‘æ–‡ä»¶ï¼Œæ— éœ€åˆå¹¶")
        return
    
    print(f"ðŸ“‹ å‘çŽ°éœ€è¦å¤„ç†çš„æ–‡ä»¶ ({len(todo_files)} ä¸ª):")
    for file in todo_files:
        print(f"  - {file}")
    
    print(f"\nðŸ“‹ ç¿»è¯‘ä¼˜å…ˆçº§ï¼šspecial_mapping > todo/new > jp_cn > temp_key_cn(data)")
    
    # 2. å¤„ç†æ¯ä¸ªæ–‡ä»¶
    all_conflicts = {}
    processed_files = []
    
    for json_filename in todo_files:
        print(f"\nðŸ“ å¤„ç†æ–‡ä»¶: {json_filename}")
        
        # æ–‡ä»¶è·¯å¾„
        todo_translated_file = os.path.join(new_files_dir, json_filename[:-5] + "_translated.json")
        old_key_cn_file = os.path.join(old_trans_dir, json_filename)
        new_key_jp_file = os.path.join(new_key_jp_dir, json_filename)
        jp_cn_file = os.path.join(jp_cn_dir, json_filename)
        output_file = os.path.join(output_dir, json_filename)
        
        # æ£€æŸ¥å¿…è¦æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(todo_translated_file):
            print(f"  âš ï¸  è·³è¿‡ {json_filename}ï¼šæ‰¾ä¸åˆ°å¯¹åº”çš„ç¿»è¯‘æ–‡ä»¶")
            continue
        
        if not os.path.exists(new_key_jp_file):
            print(f"  âš ï¸  è·³è¿‡ {json_filename}ï¼šæ‰¾ä¸åˆ°å¯¹åº”çš„ key-jp æ˜ å°„æ–‡ä»¶")
            continue
        
        # åŠ è½½ todo/new ç¿»è¯‘ï¼ˆè½¬æ¢ä¸º key-value æ ¼å¼ï¼‰
        print(f"  ðŸ“¥ åŠ è½½ todo/new ç¿»è¯‘...")
        todo_new_kv = pretranslated_to_kv_files_single(new_key_jp_dir, todo_translated_file, json_filename)
        print(f"    âœ… åŠ è½½äº† {len(todo_new_kv)} æ¡ç¿»è¯‘")
        
        # åŠ è½½special_mappingï¼ˆæ–°å¢žï¼‰
        print(f"  ðŸ“¥ åŠ è½½ç‰¹æ®Šæ˜ å°„...")
        special_mappings, special_keys = load_special_mappings(json_filename)
        if special_mappings:
            print(f"    âœ… åŠ è½½äº† {len(special_mappings)} æ¡ç‰¹æ®Šæ˜ å°„ï¼Œæ¶‰åŠ {len(special_keys)} ä¸ªå‰ç¼€")
        else:
            print(f"    â„¹ï¸  æœªæ‰¾åˆ°ç‰¹æ®Šæ˜ å°„")
        
        # åŠ è½½å…¶ä»–æ•°æ®æº
        with open(new_key_jp_file, 'r', encoding='utf-8') as f:
            new_key_jp_data = json.load(f)  # key: jp æ˜ å°„
        
        # åŠ è½½æ—§ç¿»è¯‘ (æ¥è‡ªdata)
        old_key_cn_data = {}
        if os.path.exists(old_key_cn_file):
            with open(old_key_cn_file, 'r', encoding='utf-8') as f:
                old_key_cn_data = json.load(f)
        
        # åŠ è½½ jp_cn ç¿»è¯‘æ˜ å°„
        jp_cn_data = {}
        if os.path.exists(jp_cn_file):
            with open(jp_cn_file, 'r', encoding='utf-8') as f:
                jp_cn_data = json.load(f)
        
        # åˆå¹¶ç¿»è¯‘å¹¶è®°å½•å†²çª
        final_key_cn_data = {}
        conflicts = []
        smart_exceptions = []  # æ”¶é›†æ™ºèƒ½è§£å†³çš„ç‰¹ä¾‹
        
        todo_new_count = 0
        jp_cn_count = 0
        old_count = 0
        untranslated_count = 0
        conflict_count = 0
        special_count = 0  # æ–°å¢žï¼šç‰¹æ®Šæ˜ å°„è®¡æ•°
        
        for key, jp_value in new_key_jp_data.items():
            used_translation = None
            source = None
            
            # é¦–å…ˆæ£€æŸ¥æ˜¯å¦ä¸ºç‰¹æ®Šæ˜ å°„é”®ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
            if is_special_key(key, special_keys) and key in special_mappings:
                used_translation = special_mappings[key]
                source = "special_mapping"
                special_count += 1
                final_key_cn_data[key] = used_translation
                # ç‰¹æ®Šæ˜ å°„ä¸å‚ä¸Žå†²çªæ£€æµ‹
                continue

            # æ£€æŸ¥æ‰€æœ‰å¯ç”¨çš„ç¿»è¯‘æ¥æº
            available_translations = {}
            
            # todo/new ç¿»è¯‘
            if key in todo_new_kv and todo_new_kv[key] != jp_value:  # ç¡®ä¿ä¸æ˜¯æœªç¿»è¯‘çš„æ—¥æ–‡
                available_translations["todo/new"] = todo_new_kv[key]
            
            # jp_cn ç¿»è¯‘
            if jp_value in jp_cn_data:
                available_translations["jp_cn"] = jp_cn_data[jp_value]
            
            # data ç¿»è¯‘
            if key in old_key_cn_data and old_key_cn_data[key] != jp_value:  # ç¡®ä¿ä¸æ˜¯æœªç¿»è¯‘çš„æ—¥æ–‡
                available_translations["data"] = old_key_cn_data[key]
            
            # æ™ºèƒ½å†²çªè§£å†³ï¼šå¦‚æžœjp_cnå’Œdataç¿»è¯‘ä¸€è‡´ï¼Œè€Œtodo/newä¸åŒï¼Œåˆ™ä¼˜å…ˆä½¿ç”¨jp_cn/data
            # è¿™ç§æƒ…å†µè¯´æ˜Žjp_cn/dataæ˜¯é€šç”¨ç¿»è¯‘ï¼Œè€Œtodo/newå¯èƒ½æ˜¯ç‰¹ä¾‹
            smart_resolution = False
            if (len(available_translations) >= 2 and 
                "jp_cn" in available_translations and 
                "data" in available_translations and
                "todo/new" in available_translations):
                
                jp_cn_trans = available_translations["jp_cn"]
                data_trans = available_translations["data"]
                todo_new_trans = available_translations["todo/new"]
                
                if jp_cn_trans == data_trans and jp_cn_trans != todo_new_trans:
                    # jp_cnå’Œdataä¸€è‡´ï¼Œtodo/newä¸åŒï¼šä½¿ç”¨jp_cn/dataçš„ç¿»è¯‘
                    used_translation = jp_cn_trans
                    source = "jp_cn(æ™ºèƒ½è§£å†³)"
                    jp_cn_count += 1
                    smart_resolution = True
                    
                    # æ”¶é›†ç‰¹ä¾‹ä¿¡æ¯
                    smart_exceptions.append({
                        "key": key,
                        "jp_value": jp_value,
                        "todo_new_translation": todo_new_trans,
                        "used_translation": jp_cn_trans
                    })
                    
                    print(f"    ðŸ§  æ™ºèƒ½è§£å†³å†²çª: {key[:50]}... ä½¿ç”¨é€šç”¨ç¿»è¯‘ '{jp_cn_trans}' è€Œéžç‰¹ä¾‹ '{todo_new_trans}'")
            
            # å¦‚æžœæ²¡æœ‰æ™ºèƒ½è§£å†³ï¼ŒæŒ‰åŽŸæœ‰ä¼˜å…ˆçº§é€‰æ‹©ç¿»è¯‘
            if not smart_resolution:
                if "todo/new" in available_translations:
                    used_translation = available_translations["todo/new"]
                    source = "todo/new"
                    todo_new_count += 1
                elif "jp_cn" in available_translations:
                    used_translation = available_translations["jp_cn"]
                    source = "jp_cn"
                    jp_cn_count += 1
                elif "data" in available_translations:
                    used_translation = available_translations["data"]
                    source = "data"
                    old_count += 1
                else:
                    used_translation = jp_value
                    source = "åŽŸæ–‡"
                    untranslated_count += 1
            
            final_key_cn_data[key] = used_translation
            
            # è®°å½•å†²çªï¼ˆå¤šä¸ªæ¥æºæœ‰ä¸åŒç¿»è¯‘ï¼‰
            if len(available_translations) > 1:
                unique_translations = set(available_translations.values())
                if len(unique_translations) > 1:  # ç¡®å®žæœ‰ä¸åŒçš„ç¿»è¯‘
                    conflict_record = {
                        "é”®å": key,
                        "æ—¥æ–‡åŽŸæ–‡": jp_value,
                        "å½“å‰ä½¿ç”¨": used_translation,
                        "ä½¿ç”¨æ¥æº": source
                    }
                    
                    for src, trans in available_translations.items():
                        conflict_record[f"{src}_ç¿»è¯‘"] = trans
                    
                    conflicts.append(conflict_record)
                    conflict_count += 1
        
        # ä¿å­˜åˆå¹¶åŽçš„æ–‡ä»¶
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(final_key_cn_data, f, ensure_ascii=False, indent=4)
        
        print(f"    ðŸ“Š ç¿»è¯‘ç»Ÿè®¡: special={special_count}, todo/new={todo_new_count}, jp_cn={jp_cn_count}, data={old_count}, æœªç¿»è¯‘={untranslated_count}")
        if conflict_count > 0:
            print(f"    âš ï¸  å‘çŽ°å†²çª: {conflict_count} ä¸ª")
        
        # ä¿å­˜æ™ºèƒ½è§£å†³çš„ç‰¹ä¾‹
        if smart_exceptions:
            print(f"    ðŸ§  æ™ºèƒ½è§£å†³äº† {len(smart_exceptions)} ä¸ªç‰¹ä¾‹")
            save_smart_resolved_exceptions(json_filename, smart_exceptions)
        
        # è®°å½•å†²çªå’Œå¤„ç†çš„æ–‡ä»¶
        if conflicts:
            all_conflicts[json_filename] = conflicts
        
        processed_files.append(json_filename)
        print(f"    âœ… æ–‡ä»¶å¤„ç†å®Œæˆ: {json_filename}")
    
    # 3. ç”Ÿæˆå†²çªæŠ¥å‘ŠCSV
    if all_conflicts:
        print(f"\nðŸ“‹ ç”Ÿæˆå†²çªæŠ¥å‘Š...")
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        conflicts_csv = os.path.join(conflicts_dir, f"incremental_conflicts_{timestamp}.csv")
        
        with open(conflicts_csv, 'w', encoding='utf-8-sig', newline='') as csvfile:
            fieldnames = ["æ–‡ä»¶å", "é”®å", "æ—¥æ–‡åŽŸæ–‡", "å½“å‰ä½¿ç”¨", "ä½¿ç”¨æ¥æº", "todo/new_ç¿»è¯‘", "jp_cn_ç¿»è¯‘", "data_ç¿»è¯‘"]
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            
            total_conflicts = 0
            for file_name, conflicts in all_conflicts.items():
                for conflict in conflicts:
                    row = {
                        "æ–‡ä»¶å": file_name,
                        "é”®å": conflict.get("é”®å", ""),
                        "æ—¥æ–‡åŽŸæ–‡": conflict.get("æ—¥æ–‡åŽŸæ–‡", ""),
                        "å½“å‰ä½¿ç”¨": conflict.get("å½“å‰ä½¿ç”¨", ""),
                        "ä½¿ç”¨æ¥æº": conflict.get("ä½¿ç”¨æ¥æº", ""),
                        "todo/new_ç¿»è¯‘": conflict.get("todo/new_ç¿»è¯‘", ""),
                        "jp_cn_ç¿»è¯‘": conflict.get("jp_cn_ç¿»è¯‘", ""),
                        "data_ç¿»è¯‘": conflict.get("data_ç¿»è¯‘", "")
                    }
                    writer.writerow(row)
                    total_conflicts += 1
        
        print(f"  âœ… å†²çªæŠ¥å‘Šå·²ç”Ÿæˆ: {conflicts_csv}")
        print(f"  ðŸ“Š å…±å‘çŽ° {total_conflicts} ä¸ªç¿»è¯‘å†²çª")
        print(f"  ðŸ’¡ è¯·å®¡æ ¸ CSV æ–‡ä»¶ä¸­çš„å†²çªï¼Œå¹¶å†³å®šä½¿ç”¨å“ªä¸ªç¿»è¯‘ç‰ˆæœ¬")
    else:
        print(f"\nðŸŽ‰ æœªå‘çŽ°ç¿»è¯‘å†²çªï¼")
    
    if not processed_files:
        print("\nâŒ æ²¡æœ‰æ–‡ä»¶è¢«å¤„ç†ï¼Œåˆå¹¶æ“ä½œç»“æŸ")
        return
    
    print(f"\nâœ… å¢žé‡åˆå¹¶å®Œæˆï¼")
    print(f"ðŸ“ å¤„ç†çš„æ–‡ä»¶: {len(processed_files)} ä¸ª")
    print(f"ðŸ“ åˆå¹¶ç»“æžœä¿å­˜åœ¨: {output_dir}")
    
    # 4. è¯¢é—®æ˜¯å¦ç»§ç»­æ‰§è¡Œå¢žé‡å¯¼å…¥
    user_input = input("\nðŸš€ æ˜¯å¦ç»§ç»­æ‰§è¡Œå¢žé‡å¯¼å…¥ï¼Œå°†ç¿»è¯‘æ›´æ–°åˆ° data æ–‡ä»¶å¤¹ï¼Ÿ(y/N): ").lower().strip()
    if user_input in ['y', 'yes']:
        print("ðŸ“¤ å¼€å§‹å¢žé‡å¯¼å…¥ç¿»è¯‘åˆ° data æ–‡ä»¶å¤¹...")
        
        # åªå¯¼å…¥å¤„ç†è¿‡çš„æ–‡ä»¶
        for json_filename in processed_files:
            base_file = os.path.join(gakumasu_json_dir, json_filename)
            translated_file = os.path.join(output_dir, json_filename)
            output_file = os.path.join(data_dir, json_filename)
            
            if os.path.exists(base_file) and os.path.exists(translated_file):
                import_db_json.import_main(base_file, translated_file, output_file)
                print(f"  âœ… æ›´æ–°: {json_filename}")
            else:
                print(f"  âš ï¸  è·³è¿‡ {json_filename}ï¼šç¼ºå°‘å¿…è¦æ–‡ä»¶")
        
        print("âœ… å¢žé‡å¯¼å…¥å®Œæˆï¼")
    else:
        print("â¸ï¸  è·³è¿‡å¯¼å…¥æ­¥éª¤")
        print(f"ðŸ’¡ å¦‚éœ€æ‰‹åŠ¨å¯¼å…¥ç‰¹å®šæ–‡ä»¶ï¼Œè¯·è¿è¡Œï¼š")
        print(f"   python scripts/import_db_json.py")
    
    # 5. æ˜¾ç¤ºæ‘˜è¦
    print(f"\nðŸ“‹ æ“ä½œæ‘˜è¦:")
    print(f"   - å¤„ç†æ–‡ä»¶æ•°: {len(processed_files)}")
    print(f"   - å†²çªæ–‡ä»¶æ•°: {len(all_conflicts)}")
    print(f"   - å¤„ç†çš„æ–‡ä»¶: {', '.join(processed_files[:5])}" + ("..." if len(processed_files) > 5 else ""))


if __name__ == '__main__':
    incremental_merge()
